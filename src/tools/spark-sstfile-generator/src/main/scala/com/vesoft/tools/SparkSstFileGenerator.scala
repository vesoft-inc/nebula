package com.vesoft.tools

import java.nio.charset.{Charset, UnsupportedCharsetException}

import com.vesoft.client.NativeClient
import javax.xml.bind.DatatypeConverter
import org.apache.commons.cli.{CommandLine, DefaultParser, HelpFormatter, Options, ParseException, Option => CliOption}
import org.apache.hadoop.io.BytesWritable
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.Row
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{Partitioner, SparkConf, SparkContext}
import org.slf4j.LoggerFactory

/**
  * Use spark to generate sstfile in batch, which will be ingested by Nebula engine.
  *
  * The following use cases are supported:
  *
  * <p>
  * Generate sst files from hive tables datasource, guided by a mapping file, which maps hive tables to vertexes and edges,
  * Multiple vertexes or edges may map to a single hive table, where a partition column will be used to distinguish different
  * vertex or edge.
  * The hive tables may be periodically be regenerated by your business system to reflect the latest data in so far, and may be
  * partitioned by a time column to indicate the time when generated.
  * </p>
  */
object SparkSstFileGenerator {
  private[this] val log = LoggerFactory.getLogger(this.getClass)

  /**
    * configuration key for sst file output
    */
  val SSF_OUTPUT_DIR_CONF_KEY = "nebula.graph.spark.sst.file.dir"

  /**
    * cmd line's options, which's name following the convention: input will suffix with "i", output will suffix with "o"
    */
  lazy val options: Options = {
    val dataSourceTypeInput = CliOption.builder("ti").longOpt("datasource_type_input")
      .hasArg()
      .desc("data source types support, must be among [hive|hbase|csv] for now, default=hive")
      .build

    val defaultColumnMapPolicy = CliOption.builder("ci").longOpt("default_column_mapping_policy")
      .hasArg()
      .desc("if mapping is missing, what policy to use when mapping column to property," +
        "all columns except primary_key's column will be mapped to tag's property with the same name by default")
      .build

    val mappingFileInput = CliOption.builder("mi").longOpt("mapping_file_input")
      .required()
      .hasArg()
      .desc("hive tables to nebula graph schema mapping file")
      .build

    val sstFileOutput = CliOption.builder("so").longOpt("sst_file_output")
      .required()
      .hasArg()
      .desc("where the generated sst files will be put, must be local file")
      .build

    val datePartitionKey = CliOption.builder("pi").longOpt("date_partition_input")
      .required()
      .hasArg()
      .desc("table partitioned by a date field")
      .build

    // when the newest data arrive, used in non-incremental environment
    val latestDate = CliOption.builder("di").longOpt("latest_date_input")
      .required()
      .hasArg()
      .desc("latest date to query")
      .build

    val repartitionNumber = CliOption.builder("ri").longOpt("repartition_number_input")
      .hasArg()
      .desc("repartition number")
      .build

    // may be used in some test run to prove the correctness
    val limit = CliOption.builder("li").longOpt("limit_input")
      .hasArg()
      .desc("return at most this number of edges/vertex")
      .build

    val charset = CliOption.builder("hi").longOpt("string_value_charset_input")
      .hasArg()
      .desc("when the value is of type String,what charset is used when encoded,default UTF-8")
      .build

    val opts = new Options()
    opts.addOption(defaultColumnMapPolicy)
    opts.addOption(dataSourceTypeInput)
    opts.addOption(mappingFileInput)
    opts.addOption(sstFileOutput)
    opts.addOption(datePartitionKey)
    opts.addOption(latestDate)
    opts.addOption(repartitionNumber)
    opts.addOption(charset)
    opts.addOption(limit)

  }

  // cmd line formatter when something is wrong with options
  lazy val formatter = {
    val format = new HelpFormatter
    format.setWidth(200)
    format
  }


  /**
    * partition by the partitionId part of key
    */
  class SortByKeyPartitioner(num: Int) extends Partitioner {
    override def numPartitions: Int = num

    override def getPartition(key: Any): Int = {
      (key.asInstanceOf[PartitionIdAndBytesEncoded].partitionId % numPartitions).asInstanceOf[Int]
    }
  }

  val DefaultVersion = 1

  // some prime number to redistribute records, to reduce skewness
  val DefaultRepartition = 131

  // default charset when encoding String type
  val DefaultCharset = "UTF-8"

  def main(args: Array[String]): Unit = {
    val parser = new DefaultParser

    var cmd: CommandLine = null
    try {
      cmd = parser.parse(options, args)
    }
    catch {
      case e: ParseException => {
        log.error("illegal arguments", e)
        formatter.printHelp("nebula spark sst file generator", options)
        System.exit(-1)
      }
    }

    var dataSourceTypeInput: String = cmd.getOptionValue("ti")
    if (dataSourceTypeInput == null) {
      dataSourceTypeInput = "hive"
    }

    var columnMapPolicy: String = cmd.getOptionValue("ci")
    if (columnMapPolicy == null) {
      columnMapPolicy = "hash_primary_key"
    }

    val mappingFileInput: String = cmd.getOptionValue("mi")
    var sstFileOutput: String = cmd.getOptionValue("so")
    while (sstFileOutput.endsWith("/")) {
      sstFileOutput = sstFileOutput.stripSuffix("/")
    }

    // make sure use local file system to write sst file
    if (!sstFileOutput.toLowerCase.startsWith("file://")) {
      throw new IllegalArgumentException("argument: -si --sst_file_output must be start with file://")
    }

    val limitOption: String = cmd.getOptionValue("li")
    val limit = if (limitOption != null && limitOption.nonEmpty) {
      try {
        s"LIMIT ${limitOption.toLong}"
      }
      catch {
        case _: NumberFormatException => ""
      }
    } else ""

    //when date partition is used, we should use the LATEST data
    val datePartitionKey: String = cmd.getOptionValue("pi")
    val latestDate = cmd.getOptionValue("di")

    val repartitionNumberOpt = cmd.getOptionValue("ri")
    val repartitionNumber =
      if (repartitionNumberOpt == null || repartitionNumberOpt.isEmpty) {
        DefaultRepartition
      } else {
        try {
          repartitionNumberOpt.toInt
        } catch {
          case e: Exception => {
            log.error(s"argument: -ri --repartition_number_input should be int, but found:${repartitionNumberOpt}")
            DefaultRepartition
          }
        }
      }

    // to test whether charset is supported
    val charsetOpt = cmd.getOptionValue("hi")
    val charset =
      if (charsetOpt == null || charsetOpt.isEmpty) {
        DefaultCharset
      } else {
        try {
          try {
            Charset.forName(charsetOpt)
            charsetOpt
          } catch {
            case e: UnsupportedCharsetException => {
              log.error(s"argument: -hi --string_value_charset_input is a not supported charset:${repartitionNumberOpt}")
              DefaultCharset
            }
          }
        }
      }

    // parse mapping file
    val mappingConfiguration: MappingConfiguration = MappingConfiguration(mappingFileInput)

    val sparkConf = new SparkConf().setAppName("nebula-graph-sstFileGenerator")
    val sc = new SparkContext(sparkConf)
    val sqlContext = new HiveContext(sc)

    // to pass sst file dir to SstFileOutputFormat
    sc.hadoopConfiguration.set(SSF_OUTPUT_DIR_CONF_KEY, sstFileOutput)
    // disable hadoop output compression, cause rocksdb can't recognize it
    sc.hadoopConfiguration.set(FileOutputFormat.COMPRESS, "false")

    // id generator lambda, use FNV hash for now
    //TODO: support id generator function other than FNV hash
    //TODO: handle hash collision, might cause data corruption
    val idGeneratorFunction = mappingConfiguration.keyPolicy.map(_.toLowerCase) match {
      case Some("hash_primary_key") => (key: String) => FNVHash.hash64(key)
      case Some(a@_) => throw new IllegalStateException(s"not supported key generator=${a}")
      case None => (key: String) => FNVHash.hash64(key)
    }

    // implicit ordering for BytesWritable,sstfile's key must be in strictly order
    implicit val orderingBytesWritable = new Ordering[BytesWritable] {
      override def compare(a: BytesWritable, b: BytesWritable) =
        a.compareTo(b)
    }

    //1) handle vertex, encode all column except PK column as a single Tag's properties,RDD[(PartitionID,(KeyEncoded,ValuesEncoded))]
    val tagsKVEncoded: RDD[(PartitionIdAndBytesEncoded, PartitionIdAndValueBinaryWritable)] = mappingConfiguration.tags.zipWithIndex.map {
      //tag index used as tagId
      case (tag, tagType) => {
        //all column w/o PK column
        val (allColumns, partitionCols) = validateColumns(sqlContext, tag, Seq(tag.primaryKey), Seq(tag.primaryKey), mappingConfiguration.databaseName)
        val columnExpression = {
          assert(allColumns.size > 0) // should have columns defined
          //TODO: what if allColumns is empty?
          s"${tag.primaryKey}," + allColumns.map(_.columnName).mkString(",")
        }

        val whereClause = tag.typePartitionKey.map(key => s"${key}='${tag.name}' AND ${datePartitionKey}='${latestDate}'").getOrElse(s"${datePartitionKey}='${latestDate}'")
        val tagDF = sqlContext.sql(s"SELECT ${columnExpression} FROM ${mappingConfiguration.databaseName}.${tag.tableName} WHERE ${whereClause} ${limit}") //TODO:to handle multiple partition columns' Cartesian product
        //RDD[(businessKey->values)]
        val tagKeyAndValues: RDD[(String, Seq[AnyRef])] = tagDF.map(row => {
          (row.getAs[String](tag.primaryKey) + "_" + tag.tableName, //businessId_tableName used as key before HASH
            allColumns.filter(!_.columnName.equalsIgnoreCase(tag.primaryKey)).map(col => {
              col.`type`.toUpperCase match {
                case "INTEGER" => Int.box(row.getAs[Int](col.columnName))
                case "STRING" => row.getAs[String](col.columnName).getBytes(charset) //native client should not concern string's charset
                case "FLOAT" => Float.box(row.getAs[Float](col.columnName))
                case "LONG" => Long.box(row.getAs[Long](col.columnName))
                case "DOUBLE" => Double.box(row.getAs[Double](col.columnName))
                case "BOOL" => Boolean.box(row.getAs[Boolean](col.columnName))
                case a@_ => throw new IllegalStateException(s"unsupported tag data type ${a}")
              }
            })
          )
        })

        tagKeyAndValues.map {
          case (key, values) => {
            //TODO: hash function generated signed long
            val vertexId: Long = Math.abs(idGeneratorFunction.apply(key))
            val partitionId: Int = (vertexId % mappingConfiguration.partitions).asInstanceOf[Int]
            val keyEncoded: Array[Byte] = NativeClient.createVertexKey(partitionId, vertexId, tagType, DefaultVersion)
            val valuesEncoded: Array[Byte] = NativeClient.encode(values.toArray)
            log.debug(s"Tag(partition=${partitionId}): " + DatatypeConverter.printHexBinary(keyEncoded) + " = " + DatatypeConverter.printHexBinary(valuesEncoded))
            //use vertexId as partition key
            (PartitionIdAndBytesEncoded(partitionId.toLong, new BytesWritable(keyEncoded)), new PartitionIdAndValueBinaryWritable(partitionId, new BytesWritable(valuesEncoded)))
          }
        }
      }
    }.fold(sc.emptyRDD[(PartitionIdAndBytesEncoded, PartitionIdAndValueBinaryWritable)])(_ ++ _) //TODO: too slow to concat RDD

    // implicit ordering used by PairedRDD.repartitionAndSortWithinPartitions which's key is of type PartitionIdAndBytesEncoded
    implicit def ordering[A <: PartitionIdAndBytesEncoded]: Ordering[A] = new Ordering[A] {
      override def compare(x: A, y: A): Int = {
        x.valueEncoded.compareTo(y.valueEncoded)
      }
    }

    // should generate a sub dir per partitionId in each worker node, to allow that a partition is shuffled to every worker
    // make sure each sst file's key range is not overlapping
    val sortedTagsKVEncoded = tagsKVEncoded.repartitionAndSortWithinPartitions(new SortByKeyPartitioner(repartitionNumber)).map(v => (v._1.valueEncoded, v._2))
    sortedTagsKVEncoded.saveAsNewAPIHadoopFile(sstFileOutput, classOf[BytesWritable], classOf[PartitionIdAndValueBinaryWritable], classOf[SstFileOutputFormat])

    //2)  handle edges
    val edgesKVEncoded: RDD[(PartitionIdAndBytesEncoded, PartitionIdAndValueBinaryWritable)] = mappingConfiguration.edges.zipWithIndex.map {
      //edge index used as edge_type
      case (edge, edgeType) => {
        //all column w/o PK column
        val (allColumns, partitionColumns) = validateColumns(sqlContext, edge, Seq(edge.fromForeignKeyColumn), Seq(edge.fromForeignKeyColumn, edge.toForeignKeyColumn), mappingConfiguration.databaseName)

        val columnExpression = {
          assert(allColumns.size > 0)
          s"${edge.fromForeignKeyColumn},${edge.toForeignKeyColumn}," + allColumns.map(_.columnName).mkString(",")
        }

        val whereClause = edge.typePartitionKey.map(key => s"${key}='${edge.name}' AND ${datePartitionKey}='${latestDate}'").getOrElse(s"${datePartitionKey}='${latestDate}'")

        //TODO: join FROM_COLUMN and join TO_COLUMN from the table where this columns referencing, to make sure that the claimed id really exists in the reference table.BUT with HUGE Perf penalty
        val edgeDf = sqlContext.sql(s"SELECT ${columnExpression} FROM ${mappingConfiguration.databaseName}.${edge.tableName} WHERE ${whereClause} ${limit}")
        assert(edgeDf.count() > 0)
        //RDD[(businessKey->values)]
        val edgeKeyAndValues: RDD[(String, String, Seq[AnyRef])] = edgeDf.map(row => {
          (row.getAs[String](edge.fromForeignKeyColumn), //consistent with vertexId generation logic, to make sure that vertex and its' outbound edges are in the same partition
            row.getAs[String](edge.toForeignKeyColumn), //consistent with vertexId generation logic
            allColumns.filterNot(col => (col.columnName.equalsIgnoreCase(edge.fromForeignKeyColumn) || col.columnName.equalsIgnoreCase(edge.toForeignKeyColumn))).map(col => {
              col.`type`.toUpperCase match {
                case "INTEGER" => Int.box(row.getAs[Int](col.columnName))
                case "STRING" => row.getAs[String](col.columnName).getBytes(charset)
                case "FLOAT" => Float.box(row.getAs[Float](col.columnName))
                case "LONG" => Long.box(row.getAs[Long](col.columnName))
                case "DOUBLE" => Double.box(row.getAs[Double](col.columnName))
                case "BOOL" => Boolean.box(row.getAs[Boolean](col.columnName))
                case a@_ => throw new IllegalStateException(s"unsupported edge data type ${a}")
              }
            }
            )
          )
        })

        edgeKeyAndValues.map {
          case (srcIDString, dstIdString, values) => {
            //TODO: hash function generated sign long
            val id = Math.abs(idGeneratorFunction.apply(srcIDString))
            val partitionId: Int = (id % mappingConfiguration.partitions).asInstanceOf[Int]

            val srcId = Math.abs(idGeneratorFunction.apply(srcIDString))
            val dstId = Math.abs(idGeneratorFunction.apply(dstIdString))
            // use NativeClient to generate key and encode values
            val keyEncoded = NativeClient.createEdgeKey(partitionId, srcId, edgeType.asInstanceOf[Int], -1L, dstId, DefaultVersion) //TODO: support edge ranking,like create_time desc
            val valuesEncoded: Array[Byte] = NativeClient.encode(values.toArray)
            log.debug(s"Edge(partition=${partitionId}): " + DatatypeConverter.printHexBinary(keyEncoded) + " = " + DatatypeConverter.printHexBinary(valuesEncoded))
            (PartitionIdAndBytesEncoded(id, new BytesWritable(keyEncoded)), new PartitionIdAndValueBinaryWritable(partitionId, new BytesWritable(valuesEncoded), false))
          }
        }
      }

    }.fold(sc.emptyRDD[(PartitionIdAndBytesEncoded, PartitionIdAndValueBinaryWritable)])(_ ++ _)

    val sortedEdgesKVEncoded = edgesKVEncoded.repartitionAndSortWithinPartitions(new SortByKeyPartitioner(repartitionNumber)).map(v => (v._1.valueEncoded, v._2))
    sortedEdgesKVEncoded.saveAsNewAPIHadoopFile(sstFileOutput, classOf[BytesWritable], classOf[PartitionIdAndValueBinaryWritable], classOf[SstFileOutputFormat])
  }

  /**
    * check that columns claimed in mapping configuration file are defined in db(hive)
    * and its type is compatible, when not, throw exception, return all required column definitions,
    */
  private def validateColumns(sqlContext: HiveContext, edge: WithColumnMapping, colsMustCheck: Seq[String], colsMustFilter: Seq[String], databaseName: String): (Seq[Column], Seq[String]) = {
    val descriptionDF = sqlContext.sql(s"DESC ${databaseName}.${edge.tableName}")
    // all columns' name ---> type mapping in db
    val allColumnsMapInDB: Seq[(String, String)] = descriptionDF.map {
      case Row(colName: String, colType: String, _) => {
        (colName.toUpperCase, colType.toUpperCase)
      }
    }.collect.toSeq

    val nonPartitionColumnIndex = allColumnsMapInDB.indexWhere(_._1.startsWith("#"))
    val partitionColumnIndex = allColumnsMapInDB.lastIndexWhere(_._1.startsWith("#"))
    var partitionColumns: Seq[(String, String)] = Seq.empty[(String, String)]
    if (nonPartitionColumnIndex != -1 && ((partitionColumnIndex + 1) < allColumnsMapInDB.size)) {
      partitionColumns = allColumnsMapInDB.slice(partitionColumnIndex + 1, allColumnsMapInDB.size)
    }

    //all columns except partition columns
    val allColumnMap = allColumnsMapInDB.slice(0, nonPartitionColumnIndex).toMap

    // check the claimed columns really exist in db
    colsMustCheck.map(_.toUpperCase).foreach {
      col =>
        if (allColumnMap.get(col).isEmpty) {
          throw new IllegalStateException(s"${edge.name}'s from column: ${col} not defined in table=${edge.tableName}")
        }
    }

    if (edge.columnMappings.isEmpty) {
      //only (from,to) columns are checked, but all columns should be returned
      (allColumnMap.filter(!partitionColumns.contains(_)).filter(!colsMustFilter.contains(_)).map {
        case (colName, colType) => {
          Column(colName, colName, colType) // propertyName default=colName
        }
      }.toSeq, partitionColumns.map(_._1))
    }
    else {
      // tag/edge's columnMappings should be checked and returned
      val columnMappings = edge.columnMappings.get
      val notValid = columnMappings.filter(
        col => {
          val typeInDb = allColumnMap.get(col.columnName.toUpperCase)
          typeInDb.isEmpty || !DataTypeCompatibility.isCompatible(col.`type`, typeInDb.get)
        }
      ).map {
        case col => s"name=${col.columnName},type=${col.`type`}"
      }

      if (notValid.nonEmpty) {
        throw new IllegalStateException(s"${edge.name}'s columns: ${notValid.mkString("\t")} not defined in or compatible with db's definitions")
      }
      else {
        (columnMappings, partitionColumns.map(_._1))
      }
    }
  }
}
