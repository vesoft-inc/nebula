Generate sst files from hive tables datasource, guided by a mapping file, which maps hive tables to vertexes and edges,
Multiple vertexes or edges may map to a single hive table, where a partition column will be used to distinguish different
vertex or edge.
The hive tables may be periodically be regenerated by upstream system to reflect the latest data in so far, and may be
partitioned by a time column to indicate the time when data are generated.

# Environment
component|version
---|---
spark|1.6.2
hadoop|2.7.4
jdk|1.8+
scala|2.10.5
sbt|1.2.8


# Spark-submit command line reference
This is what we used in production environment: 
```bash
/soft/spark/bin/spark-submit --master yarn --queue fmprod --conf spark.executor.instances=24 --conf spark.executor.memory=90g --conf spark.executor.cores=2  --conf spark.executorEnv.LD_LIBRARY_PATH='/soft/server/nebula_native_client:/usr/local/lib:/usr/local/lib64' --conf spark.driver.extraJavaOptions='-Djava.library.path=/soft/server/nebula_native_client/:/usr/local/lib64:/usr/local/lib' --class com.vesoft.tools.SparkSstFileGenerator --files mapping.json nebula-spark-sstfile-generator.jar -li "2019-05-06" -mi mapping.json -pi dt -so nebula_output
```
The application options are described as following.

# Spark application command line reference 
We keep a convention when naming the option,those suffix with _i_ will be an INPUT type option, while those suffix with _o_ will be an OUTPUT type option 

```bash
usage: nebula spark sst file generator
 -ci,--default_column_mapping_policy <arg>   if mapping is missing, what policy to use when mapping column to property,all columns except primary_key's column will be mapped to tag's property with the same name by default
 -di,--latest_date_input <arg>               latest date to query, date format YYYY-MM-dd
 -hi,--string_value_charset_input <arg>      when the value is of type String,what charset is used when encoded,default to UTF-8
 -li,--limit_input <arg>                     return at most this number of edges/vertex
 -mi,--mapping_file_input <arg>              hive tables to nebula graph schema mapping file
 -pi,--date_partition_input <arg>            table partitioned by a date field
 -ri,--repartition_number_input <arg>        repartition number
 -so,--sst_file_output <arg>                 where the generated sst files will be put, must be local file
 -ti,--datasource_type_input <arg>           data source types support, support only hive in so far, default=hive
```

# Mapping file format

When used in production, should be modified according to your scenario, and strip the comment, for the json parser used here lacks the capability of handling comments
This mapping file should be provided through spark-submit command line argument --files, which will be loaded by spark worker.

```json
{
  // [required], graphspace name
  "space_one": {
    // [optional], encoding algorithm used when convert business key to vertex key, only support `hash_primary_key` in so far, could be omitted, or used as it's
    "key_policy": "hash_primary_key",
    // [required],graphspace partition number
    "partitions": 3,
    // [optional,but a mapping file should at least contain one tag or edge],vertex tags' mapping array
    "tags": [
      {
        // [required],tag's datasource table name
        "table_name": "dmt_risk_graph_idmp_node_s_d",
        // [required],tag's name
        "tag_name": "mac",
        // [required],date partition key used by datasource table
        "date_partition_key": "dt",
        // [optional],if one datasource table maps to multiple tags, which column would be used as discrimination columns
        "type_partition_key": "flag",
        // [required],tag's datasource table's pimary key column 
        "primary_key": "node",
        // [optional], tag's property mappings, not all columns in source table will be used as properties. When omitted, all columns will be used as its properties except those specified by primary_key,date_partition_key and type_partition_key
        "mappings": [
          {
            // [required],tag's property name 
            "src_pri_value": {
              // [required],datasource table column's name, which hold this property's value
              "name": "src_pri_value",
              // [optional],datasource table column's type, will be used to do type conversion to graph's native data type, default to string. The charset used default to UTF-8, could be changed through command line option '--string_value_charset_input','-hi' for short 
              "type": "string"
            }
          }
        ]
      },
      {
        "table_name": "dmt_risk_graph_idmp_node_s_d",
        "tag_name": "user_pin",
        "date_partition_key": "dt",
        "type_partition_key": "flag",
        "primary_key": "node",
        "mappings": [
          {
            "src_pri_value": {
              "name": "src_pri_value",
              "type": "string"
            }
          }
        ]
      }
    ],
    // [optional,but a mapping file should at least contain one tag or edge],edges' mapping array
    "edges": [
      {
        // [required],edge's datasource table name
        "table_name": "dmt_risk_graph_idmp_edge_s_d",
        // [required],edge's name
        "edge_name": "pin2mac",
        // same as Tag's
        "date_partition_key": "dt",
        // same as Tag's
        "type_partition_key": "flag",
        // [required],edge's FROM column
        "from_foreign_key_column": "from_node",
        // [required],what edge's FROM column represent, would be used as a cross reference
        "from_tag": "user_pin",
        // [required],edge's TO column
        "to_foreign_key_column": "to_node",
        // [required],what edge's TO column represent, would be used as a cross reference
        "to_tag": "mac",
        // [optional], edge's property mappings,not all columns in source table will be used as properties. When omitted, all columns will be used as its properties except those specified by from_foreign_key_column,to_foreign_key_column„ÄÅdate_partition_key and type_partition_key
        "mappings": [
          {
            // [required],edge's property name 
            "src_pri_value": {
              // [required],datasource table column's name, which hold this property's value
              "name": "src_pri_value",
              // [optional],datasource table column's type, will be used to do type conversion to graph's native data type, default to string. The charset used default to UTF-8, could be changed through command line option '--string_value_charset_input','-hi' for short 
              "type": "string"
            }
          }
        ]
      }
    ]
  }
}
```

# FAQ
## How to use libnebula-native-client.so under CentOS6.5(2.6.32-431 x86-64)

1. build rocksdbjava natively under CentOS6.5, which will generate librocksdbjni-linux64.so
```bash
DEBUG_LEVEL=0 make shared_lib
DEBUG_LEVEL=0 make rocksdbjava
```
_make sure to keep consistent with DEBUG_LEVEL, or there will be some link error like `symbol not found`
2. run `sbt assembly` to package this project to a spark job jar, which is default named: `nebula-spark-sstfile-generator.jar`
3. run `jar uvf nebula-spark-sstfile-generator.jar librocksdbjni-linux64.so libnebula_native_client.so` to replace the `*.so` files packaged inside the dependency org.rocksdb:rocksdbjni:5.17.2,or some error like following will occur when spark-submit:
```
*** glibc detected *** /soft/java/bin/java: free(): invalid pointer: 0x00007f7985b9f0a0 ***
======= Backtrace: =========
/lib64/libc.so.6(+0x75f4e)[0x7f7c7d5e6f4e]
/lib64/libc.so.6(+0x78c5d)[0x7f7c7d5e9c5d]
/tmp/librocksdbjni3419235685305324910.so(_ZN7rocksdb10EnvOptionsC1Ev+0x578)[0x7f79431ff908]
/tmp/librocksdbjni3419235685305324910.so(Java_org_rocksdb_EnvOptions_newEnvOptions+0x1c)[0x7f7943044dbc]
[0x7f7c689c1747]
```

# TODO
1. Sst files are written to worker's local file system, for rocksdb lacks the capability of writing remote filesystem like HDFS, need a way for those file to be accessed by Nebula _load_ command 
2. Add database_name property to graphspace level and tag/edge level, which the latter will override the former when provided in both levels
3. Schema column definitions' order is important, keep it when parsing mapping file and when encoding
4. Integrated build with maven or cmake, where this spark assembly should be build after nebula native client
5. To handle following situation: different tables share a common Tag, like a tag with properties of (start_time, end_time)

